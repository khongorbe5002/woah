{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e46054c1",
   "metadata": {},
   "source": [
    "# Export YOLOv10n to ONNX — Colab\n",
    "This notebook walks through exporting a YOLO `.pt` model to ONNX using a Colab environment (GPU recommended). Follow the cells in order: install dependencies, upload model, export (via helper script or direct API), validate ONNX, and download the result.\n",
    "\n",
    "**Select Runtime → Change runtime type → Hardware accelerator: GPU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0005ab4",
   "metadata": {},
   "source": [
    "## Section 1 — Setup & Imports\n",
    "Install required packages and import standard libraries. Run the cell below to set up the Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd65d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run this cell)\n",
    "!pip install --upgrade pip\n",
    "!pip install ultralytics onnx onnxruntime onnxsim --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b8d71",
   "metadata": {},
   "source": [
    "## Section 2 — Environment & dependency checks\n",
    "Verify Python version and that key packages are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4d6ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick environment checks\n",
    "import sys\n",
    "print('Python', sys.version)\n",
    "\n",
    "# show installed ultralytics if present\n",
    "!python -c \"import ultralytics; print('ultralytics', ultralytics.__version__)\" || echo 'ultralytics not installed'\n",
    "\n",
    "# List packages of interest\n",
    "!python -m pip show onnx onnxruntime onnxsim || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d812675",
   "metadata": {},
   "source": [
    "## Section 3 — Upload/YAML & inspect model\n",
    "Upload a `.pt` model from your machine or download it via URL. The cell below uses `files.upload()` to let you select a file interactively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fdd27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload .pt model interactively\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "for fname in uploaded:\n",
    "    print('Uploaded:', fname)\n",
    "\n",
    "# Optionally check filesize and list\n",
    "import os\n",
    "for fname in uploaded:\n",
    "    print(fname, os.path.getsize(fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd3025b",
   "metadata": {},
   "source": [
    "## Sections 4-6 — Preprocessing, Features, EDA (Placeholders)\n",
    "These sections are included as a template for broader data workflows. For model export they are typically not required, but we keep short examples and placeholders to show how you'd document and test these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3866ad9",
   "metadata": {},
   "source": [
    "## Section 7 — Export model to ONNX (helper script)\n",
    "Use the repo's helper export script to convert your uploaded `.pt` to `.onnx`.\n",
    "\n",
    "Run the cell below to run: `python scripts/export_yolo_to_onnx.py --model yolov10n.pt --output yolov10n.onnx --opset 12 --simplify` (replace filenames as needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c86b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the helper script (adjust arguments if your model has another name)\n",
    "!python scripts/export_yolo_to_onnx.py --model yolov10n.pt --output yolov10n.onnx --opset 12 --simplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c349fe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct export via ultralytics API (alternative)\n",
    "try:\n",
    "    from ultralytics import YOLO\n",
    "    print('ultralytics available:', YOLO)\n",
    "    model = YOLO('yolov10n.pt')\n",
    "    print('Exporting using ultralytics API...')\n",
    "    model.export(format='onnx', opset=12, imgsz=640, simplify=True)\n",
    "    print('API export finished.')\n",
    "except Exception as e:\n",
    "    print('Ultralytics API export failed:', e)\n",
    "    print('You can still try the helper script or adjust opset/simplify.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa74212",
   "metadata": {},
   "source": [
    "## Section 8 — Validate ONNX\n",
    "Check the exported ONNX file with `onnx.checker` and ensure `onnxruntime` can load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX validation\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "onnx_path = 'yolov10n.onnx'\n",
    "try:\n",
    "    model = onnx.load(onnx_path)\n",
    "    onnx.checker.check_model(model)\n",
    "    print('ONNX check: OK')\n",
    "    sess = ort.InferenceSession(onnx_path)\n",
    "    print('onnxruntime providers:', sess.get_providers())\n",
    "except Exception as e:\n",
    "    print('ONNX validation failed:', e)\n",
    "    print('Check the export output and try different opset/simplify settings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7417ef6",
   "metadata": {},
   "source": [
    "## Section 9 — Save & export artifacts\n",
    "Download the ONNX or copy it to your Google Drive for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the ONNX file to your local machine\n",
    "from google.colab import files\n",
    "files.download('yolov10n.onnx')\n",
    "\n",
    "# OR: copy to Google Drive (uncomment to use)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# !cp yolov10n.onnx /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba3a3c0",
   "metadata": {},
   "source": [
    "## Section 10 — Unit tests & reproducibility\n",
    "Example: a short pytest test ensuring ONNX loads and that a basic forward pass runs (uses a random input for smoke test).\n",
    "\n",
    "```python\n",
    "# tests/test_onnx_load.py\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "def test_onnx_load_and_session():\n",
    "    model = onnx.load('yolov10n.onnx')\n",
    "    onnx.checker.check_model(model)\n",
    "    sess = ort.InferenceSession('yolov10n.onnx')\n",
    "    assert 'CPUExecutionProvider' in sess.get_providers()\n",
    "```\n",
    "\n",
    "Run tests with `pytest -q` after saving the file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a7cd94",
   "metadata": {},
   "source": [
    "## Section 11 — Running & debugging in VS Code\n",
    "- You can open this notebook in VS Code and run cells interactively.\n",
    "- Use the Jupyter extension or the built-in Notebook UI.\n",
    "- To debug cells in VS Code set breakpoints in the cell and use the 'Run by line' or the Debug Cell action.\n",
    "\n",
    "**Open in Colab**: you can upload this notebook to Colab or use `Open in Colab` links if you publish the repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7397a6c",
   "metadata": {},
   "source": [
    "---\n",
    "## Troubleshooting & Tips\n",
    "- If export fails with errors from `torch` or `ultralytics`, run the export on an x86 machine or use Colab GPU runtime (this notebook).\n",
    "- Try opset 11/12 if you see unsupported operator errors.\n",
    "- If `onnx.checker` fails, try exporting without `simplify=True` to inspect raw errors.\n",
    "- Use `onnxruntime` CPU provider for quick checks; GPU provider requires additional setup.\n",
    "\n",
    "If you want, run the cells now. If you'd like, I can also add a short example to run inference with the ONNX model on a sample image inside this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c13ca8",
   "metadata": {},
   "source": [
    "## Optional: Run an ONNX inference example on a sample image\n",
    "Upload a sample image and run a small inference + visualization step to verify the exported ONNX works on a real image.\n",
    "\n",
    "Notes:\n",
    "- The ONNX output format may vary between model exports. The cell below attempts a robust parsing for common YOLO-style outputs (xywh + confidence + class scores). If your model uses a different layout, adapt the parsing accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15bcb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample image: prefer the repo-provided sample if available\n",
    "import os\n",
    "sample_b64 = 'data/sample_image.b64'\n",
    "sample_png = 'data/sample_image.png'\n",
    "\n",
    "if os.path.exists(sample_png):\n",
    "    img_path = sample_png\n",
    "    print('Using repo sample image:', img_path)\n",
    "elif os.path.exists(sample_b64):\n",
    "    print('Extracting sample image from base64...')\n",
    "    # run the helper to write the PNG\n",
    "    import subprocess\n",
    "    subprocess.run([\"python\", \"scripts/extract_sample_image.py\"], check=False)\n",
    "    if os.path.exists(sample_png):\n",
    "        img_path = sample_png\n",
    "        print('Extracted and using:', img_path)\n",
    "    else:\n",
    "        print('Extraction failed; falling back to upload prompt')\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()\n",
    "        if len(uploaded) == 0:\n",
    "            raise SystemExit('No image uploaded.')\n",
    "        img_path = next(iter(uploaded.keys()))\n",
    "        print('Using uploaded image:', img_path)\n",
    "else:\n",
    "    from google.colab import files\n",
    "    uploaded = files.upload()\n",
    "    if len(uploaded) == 0:\n",
    "        raise SystemExit('No image uploaded.')\n",
    "    img_path = next(iter(uploaded.keys()))\n",
    "    print('Using uploaded image:', img_path)\n",
    "\n",
    "# Show the image\n",
    "from PIL import Image\n",
    "Image.open(img_path).resize((400,400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb4ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ONNX inference and visualize detections\n",
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load image\n",
    "img_bgr = cv2.imread(img_path)\n",
    "if img_bgr is None:\n",
    "    raise SystemExit('Failed to read image')\n",
    "img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "orig_h, orig_w = img_rgb.shape[:2]\n",
    "\n",
    "# Settings\n",
    "input_size = 640  # match the size used for export\n",
    "conf_thresh = 0.4\n",
    "\n",
    "# Preprocess: resize (no letterbox for simplicity) and normalize\n",
    "img_resized = cv2.resize(img_rgb, (input_size, input_size))\n",
    "input_tensor = img_resized.astype(np.float32) / 255.0\n",
    "input_tensor = np.transpose(input_tensor, (2, 0, 1))[None, :]\n",
    "\n",
    "# Run session\n",
    "sess = ort.InferenceSession('yolov10n.onnx')\n",
    "input_name = sess.get_inputs()[0].name\n",
    "outputs = sess.run(None, {input_name: input_tensor})\n",
    "print('Number of outputs:', len(outputs))\n",
    "for i, out in enumerate(outputs):\n",
    "    print(f'output[{i}] shape:', out.shape)\n",
    "\n",
    "# Try to parse the first output into detections (common YOLO layouts)\n",
    "out = outputs[0]\n",
    "if out.ndim == 3:  # (1, N, C)\n",
    "    dets = out[0]\n",
    "elif out.ndim == 2:  # (N, C)\n",
    "    dets = out\n",
    "else:\n",
    "    print('Unexpected ONNX output shape; raw output shown above. Inspect and adapt parsing.')\n",
    "    dets = None\n",
    "\n",
    "coco_names = [\n",
    "    'person','bicycle','car','motorcycle','airplane','bus','train','truck','boat','traffic light','fire hydrant','stop sign','parking meter','bench','bird','cat','dog','horse','sheep','cow','elephant','bear','zebra','giraffe','backpack','umbrella','handbag','tie','suitcase','frisbee','skis','snowboard','sports ball','kite','baseball bat','baseball glove','skateboard','surfboard','tennis racket','bottle','wine glass','cup','fork','knife','spoon','bowl','banana','apple','sandwich','orange','broccoli','carrot','hot dog','pizza','donut','cake','chair','couch','potted plant','bed','dining table','toilet','tv','laptop','mouse','remote','keyboard','cell phone','microwave','oven','toaster','sink','refrigerator','book','clock','vase','scissors','teddy bear','hair drier','toothbrush'\n",
    "]\n",
    "\n",
    "vis = img_rgb.copy()\n",
    "if dets is not None and dets.shape[1] >= 6:\n",
    "    # Interpret as: x_center, y_center, w, h, conf, class_scores... or x,y,w,h,conf,class_id\n",
    "    x = dets[:, 0]\n",
    "    y = dets[:, 1]\n",
    "    w = dets[:, 2]\n",
    "    h = dets[:, 3]\n",
    "    conf = dets[:, 4]\n",
    "\n",
    "    if dets.shape[1] > 6:\n",
    "        class_scores = dets[:, 5:]\n",
    "        class_idx = np.argmax(class_scores, axis=1)\n",
    "        class_score = np.max(class_scores, axis=1)\n",
    "        final_score = conf * class_score\n",
    "    else:\n",
    "        class_idx = dets[:, 5].astype(int)\n",
    "        final_score = conf\n",
    "\n",
    "    # If coordinates are normalized (<=1), convert to pixels in resized image\n",
    "    if np.max(w) <= 1.0 + 1e-6:\n",
    "        x *= input_size\n",
    "        y *= input_size\n",
    "        w *= input_size\n",
    "        h *= input_size\n",
    "\n",
    "    x1 = x - w / 2\n",
    "    y1 = y - h / 2\n",
    "    x2 = x + w / 2\n",
    "    y2 = y + h / 2\n",
    "\n",
    "    # Scale back to original image size\n",
    "    sx = orig_w / input_size\n",
    "    sy = orig_h / input_size\n",
    "    x1 *= sx; x2 *= sx; y1 *= sy; y2 *= sy\n",
    "\n",
    "    # Filter and draw\n",
    "    indices = np.where(final_score > conf_thresh)[0]\n",
    "    print(f'Filtered {len(indices)} detections (score > {conf_thresh})')\n",
    "    for i in indices:\n",
    "        xa, ya, xb, yb = int(x1[i]), int(y1[i]), int(x2[i]), int(y2[i])\n",
    "        cls = int(class_idx[i]) if class_idx is not None else -1\n",
    "        label = coco_names[cls] if (0 <= cls < len(coco_names)) else str(cls)\n",
    "        sc = float(final_score[i])\n",
    "        cv2.rectangle(vis, (xa, ya), (xb, yb), (0, 255, 0), 2)\n",
    "        cv2.putText(vis, f'{label} {sc:.2f}', (xa, max(ya - 6, 0)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,0), 2)\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(vis)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Detections array missing or not in expected format. See printed output shapes and adapt parsing as needed.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
